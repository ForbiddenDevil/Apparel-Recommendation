{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of data points : 183138 No. of features/variables : 19\n"
     ]
    }
   ],
   "source": [
    "# Loading the data using pandas' read_json function...\n",
    "data = pd.read_json('tops_fashion.json')\n",
    "\n",
    "# Getting the number of rows and columns in the dataframe...\n",
    "print(\"No. of data points :\", data.shape[0], \"No. of features/variables :\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'author', 'availability', 'availability_type', 'brand', 'color',\n",
       "       'editorial_reivew', 'editorial_review', 'formatted_price',\n",
       "       'large_image_url', 'manufacturer', 'medium_image_url', 'model',\n",
       "       'product_type_name', 'publisher', 'reviews', 'sku', 'small_image_url',\n",
       "       'title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the features in the dataframe...\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of data points : 183138 No. of features/variables : 7\n"
     ]
    }
   ],
   "source": [
    "# Using only 7 features and discarding remaining 12 from the dataframe...\n",
    "data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]\n",
    "\n",
    "# Again getting the number of rows and columns in the dataframe...\n",
    "print(\"No. of data points :\", data.shape[0], \"No. of features/variables :\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B016I2TS4W</td>\n",
       "      <td>FNC7C</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Minions Como Superheroes Ironman Long Sleeve R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01N49AI08</td>\n",
       "      <td>FIG Clothing</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>FIG Clothing Womens Izo Tunic</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B01JDPCOHO</td>\n",
       "      <td>FIG Clothing</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>FIG Clothing Womens Won Top</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B01N19U5H5</td>\n",
       "      <td>Focal18</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Focal18 Sailor Collar Bubble Sleeve Blouse Shi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B004GSI2OS</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>Onyx Black/ Stone</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Featherlite Ladies' Long Sleeve Stain Resistan...</td>\n",
       "      <td>$26.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin         brand              color  \\\n",
       "0  B016I2TS4W         FNC7C               None   \n",
       "1  B01N49AI08  FIG Clothing               None   \n",
       "2  B01JDPCOHO  FIG Clothing               None   \n",
       "3  B01N19U5H5       Focal18               None   \n",
       "4  B004GSI2OS   FeatherLite  Onyx Black/ Stone   \n",
       "\n",
       "                                    medium_image_url product_type_name  \\\n",
       "0  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "1  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "2  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "3  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "4  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                               title formatted_price  \n",
       "0  Minions Como Superheroes Ironman Long Sleeve R...            None  \n",
       "1                      FIG Clothing Womens Izo Tunic            None  \n",
       "2                        FIG Clothing Womens Won Top            None  \n",
       "3  Focal18 Sailor Collar Bubble Sleeve Blouse Shi...            None  \n",
       "4  Featherlite Ladies' Long Sleeve Stain Resistan...          $26.26  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's have a look at the data...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              asin   brand  color  \\\n",
      "count       183138  182987  64956   \n",
      "unique      183138   10577   7380   \n",
      "top     B01N9MM0J8    Zago  Black   \n",
      "freq             1     223  13207   \n",
      "\n",
      "                                         medium_image_url product_type_name  \\\n",
      "count                                              183138            183138   \n",
      "unique                                             170782                72   \n",
      "top     https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
      "freq                                                   23            167794   \n",
      "\n",
      "                                                    title formatted_price  \n",
      "count                                              183138           28395  \n",
      "unique                                             175985            3135  \n",
      "top     Nakoda Cotton Self Print Straight Kurti For Women          $19.99  \n",
      "freq                                                   77             945  \n"
     ]
    }
   ],
   "source": [
    "#Let's look at some statistics of data...\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating price=NULL : 28395\n",
      "Number of data points After eliminating color=NULL : 28385\n"
     ]
    }
   ],
   "source": [
    "#You can skip the below four lines to run the model on the entire dataset...\n",
    "\n",
    "#Use this to remove items with no price given...\n",
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])\n",
    "\n",
    "#Use this to remove items with no color given...\n",
    "data =data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating color=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO\\n\\nfor index, row in images.iterrows():\\n        url = row['large_image_url']\\n        response = requests.get(url)\\n        img = Image.open(BytesIO(response.content))\\n        img.save('images/183k_images/'+row['asin']+'.jpeg')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can download all the images if necessary using the following code...\n",
    "\n",
    "'''\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "for index, row in images.iterrows():\n",
    "        url = row['large_image_url']\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img.save('images/183k_images/'+row['asin']+'.jpeg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325\n"
     ]
    }
   ],
   "source": [
    "#Finding the number of products which have exactly the same title...\n",
    "print(sum(data.duplicated('title')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of short titles:  27949\n"
     ]
    }
   ],
   "source": [
    "#If title has ver few words, its of little use, so let's remove them...\n",
    "data_sorted = data[data['title'].apply(lambda x: len(x.split()) > 4)]\n",
    "print(\"After removal of short titles: \", data_sorted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118987</th>\n",
       "      <td>B008D30AGK</td>\n",
       "      <td>Out+of+Print+Clothing</td>\n",
       "      <td>Multicolored</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>\"1984\" Retro Book Cover Women's SLim Fit T-Shi...</td>\n",
       "      <td>$7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78827</th>\n",
       "      <td>B003IDE8XQ</td>\n",
       "      <td>Maggie's Organics</td>\n",
       "      <td>Grey</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>HOME</td>\n",
       "      <td>\"Camisoles Grey - Medium Fair Labor, 1 pc\"</td>\n",
       "      <td>$18.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109599</th>\n",
       "      <td>B00KI3VDXM</td>\n",
       "      <td>Crazy4Bling</td>\n",
       "      <td>Purple</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>\"I Wanna Be Adored\" Long Sleeve Top with Shred...</td>\n",
       "      <td>$39.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>B073SKNQHD</td>\n",
       "      <td>The Workout Princess</td>\n",
       "      <td>Premium Heather Gray</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>\"I Workout To Burn off The Crazy\", Tri Blend T...</td>\n",
       "      <td>$24.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12050</th>\n",
       "      <td>B06WRW8RQ1</td>\n",
       "      <td>AJ</td>\n",
       "      <td>Black</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>\"Life is a Journey\" Self-Help DIY T-Shirt (Wom...</td>\n",
       "      <td>$9.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin                  brand                 color  \\\n",
       "118987  B008D30AGK  Out+of+Print+Clothing          Multicolored   \n",
       "78827   B003IDE8XQ      Maggie's Organics                  Grey   \n",
       "109599  B00KI3VDXM            Crazy4Bling                Purple   \n",
       "40451   B073SKNQHD   The Workout Princess  Premium Heather Gray   \n",
       "12050   B06WRW8RQ1                     AJ                 Black   \n",
       "\n",
       "                                         medium_image_url product_type_name  \\\n",
       "118987  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "78827   https://images-na.ssl-images-amazon.com/images...              HOME   \n",
       "109599  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "40451   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "12050   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                                    title formatted_price  \n",
       "118987  \"1984\" Retro Book Cover Women's SLim Fit T-Shi...           $7.51  \n",
       "78827          \"Camisoles Grey - Medium Fair Labor, 1 pc\"          $18.99  \n",
       "109599  \"I Wanna Be Adored\" Long Sleeve Top with Shred...          $39.99  \n",
       "40451   \"I Workout To Burn off The Crazy\", Tri Blend T...          $24.99  \n",
       "12050   \"Life is a Journey\" Self-Help DIY T-Shirt (Wom...           $9.38  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Soting data according to title in alphbetical order...\n",
    "data_sorted.  sort_values('title', inplace = True)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "stage1_dedupe_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    previous_i = i\n",
    "    #Store the list of words of ith string in a...\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "    j = i+1\n",
    "    while j < num_data_points:\n",
    "        #Store the list of words of jth string in b...\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "        #Store the maximum of lengths of a and b...\n",
    "        max_length = max(len(a), len(b))\n",
    "        #Count is used to store number of words matched...\n",
    "        count = 0\n",
    "        #itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings...\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "        if (max_length - count) > 2:\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "            if j == num_data_points-1:\n",
    "                stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points :  17587\n"
     ]
    }
   ],
   "source": [
    "#Here, we actually remove duplicates which differ only at the end...\n",
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]\n",
    "#Printing number of products remaining after removal of duplicates which differ only at the end...\n",
    "print('Number of data points : ', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is bruteforcing method and takes significant amount of time...\n",
    "\n",
    "indices = []\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "\n",
    "stage2_dedupe_asins = []\n",
    "while len(indices)!=0:\n",
    "    i = indices.pop()\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    #Consider the first apperal's title\n",
    "    a = data['title'].loc[i].split()\n",
    "    #Store the list of words of ith string in a...\n",
    "    for j in indices:\n",
    "        \n",
    "        b = data['title'].loc[j].split()\n",
    "        #Store the list of words of jth string in b...\n",
    "        \n",
    "        length = max(len(a),len(b))\n",
    "        \n",
    "        #Count signifies number of words matched in both the strings...\n",
    "        count  = 0\n",
    "\n",
    "        #itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        #Example: a =['a', 'b', 'c', 'd']\n",
    "        #b = ['a', 'b', 'd']\n",
    "        #itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "\n",
    "        #If the number of words in which both strings differ are < 3 , we are consider those two apperals as same and hence ignore them...\n",
    "        if (length - count) < 3:\n",
    "            indices.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points after stage two of dedupe:  16437\n"
     ]
    }
   ],
   "source": [
    "#We'll subset our data by result of above loop... \n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]\n",
    "print('Number of data points after stage two of dedupe: ',data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle complete... You may stop the job\n"
     ]
    }
   ],
   "source": [
    "#We'll store this state of data so that we don't have to do all the processing again...\n",
    "data.to_pickle('clean_data')\n",
    "print(\"Pickle complete... You may stop the job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"clean_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove the stop words in the titles...\n",
    "#We'll use stop words from nltk library...\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            #This removes any special characters, if any...\n",
    "            word = (\"\".join(x for x in words if x.isalnum()))\n",
    "            #This converts all words to lowercase...\n",
    "            word = word.lower()\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        data[column][index] = string\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We'll take each title and remove its stop words using above function...\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing(row[\"title\"], index, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some utility functions...\n",
    "\n",
    "#Display an image\n",
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it in notebook \n",
    "    plt.imshow(img)\n",
    "  \n",
    "#plotting code to understand the algorithm's decision.\n",
    "def plot_heatmap(keys, values, labels, url, text):\n",
    "        # keys: list of words of recommended title\n",
    "        # values: len(values) ==  len(keys), values(i) represents the occurence of the word keys(i)\n",
    "        # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "                # if model == 'bag of words': labels(i) = values(i)\n",
    "                # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "                # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "        # url : apparel's url\n",
    "\n",
    "        # we will devide the whole figure into two parts\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1]) \n",
    "        fig = plt.figure(figsize=(25,3))\n",
    "        \n",
    "        # 1st, ploting heat map that represents the count of commonly ocurred words in title2\n",
    "        ax = plt.subplot(gs[0])\n",
    "        # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not\n",
    "        ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n",
    "        ax.set_xticklabels(keys) # set that axis labels as the words of title\n",
    "        ax.set_title(text) # apparel title\n",
    "        \n",
    "        # 2nd, plotting image of the the apparel\n",
    "        ax = plt.subplot(gs[1])\n",
    "        # we don't want any grid lines for image and no labels on x-axis and y-axis\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # we call dispaly_img based with paramete url\n",
    "        display_img(url, ax, fig)\n",
    "        \n",
    "        # displays combine figure ( heat map and image together)\n",
    "        plt.show()\n",
    "    \n",
    "def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):\n",
    "\n",
    "    # doc_id : index of the title1\n",
    "    # vec1 : input apparels's vector, it is of a dict type {word:count}\n",
    "    # vec2 : recommended apparels's vector, it is of a dict type {word:count}\n",
    "    # url : apparels image url\n",
    "    # text: title of recomonded apparel (used to keep title of image)\n",
    "    # model, it can be any of the models, \n",
    "        # 1. bag_of_words\n",
    "        # 2. tfidf\n",
    "        # 3. idf\n",
    "\n",
    "    # we find the common words in both titles, because these only words contribute to the distance between two title vec's\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys()) \n",
    "\n",
    "    # we set the values of non intersecting words to zero, this is just to show the difference in heatmap\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "\n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "    #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "        # if model == 'bag of words': labels(i) = values(i)\n",
    "        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in  tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in  idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    plot_heatmap(keys, values, labels, url, text)\n",
    "\n",
    "\n",
    "# this function gets a list of wrods along with the frequency of each \n",
    "# word given \"text\"\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n",
    "    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n",
    "\n",
    "\n",
    "\n",
    "def get_result(doc_id, content_a, content_b, url, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    \n",
    "    # vector1 = dict{word11:#count, word12:#count, etc.}\n",
    "    vector1 = text_to_vector(text1)\n",
    "\n",
    "    # vector1 = dict{word21:#count, word22:#count, etc.}\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16437, 12683)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the title of each data point in a n dimensional vector...\n",
    "#Where n is the total number of distinct words in all titles combined...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "title_vectorizer = CountVectorizer()\n",
    "title_features   = title_vectorizer.fit_transform(data['title'])\n",
    "title_features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
